#!/usr/bin/env python3
"""Crawl a website, snapshot pages, and generate PDF/Markdown docs.

How to use:
1. Install dependencies:
   pip install beautifulsoup4 fpdf Pillow markdownify playwright
   playwright install  # downloads headless browsers
2. Download the DejaVuSans font for Unicode PDF support:
   https://github.com/dejavu-fonts/dejavu-fonts/raw/version_2_37/ttf/DejaVuSans.ttf
   (If using the ZIP release, extract `DejaVuSans.ttf` from the archive.)
3. Set `FONT_PATH` below to the downloaded file's location.
4. Edit BASE_URL to point to your server.
5. Run: python scripts/site_snapshot.py
6. Output appears in the "site_manual" directory with screenshots,
   per-page Markdown files, and a combined PDF manual.

Pillow provides image support for FPDF; without it, PDFs are generated without screenshots.
The crawler uses Playwright to render pages so that links generated by JavaScript are
discovered correctly.
"""

import asyncio
import logging
import re
from collections import deque
from pathlib import Path
from urllib.parse import urljoin, urlparse

from bs4 import BeautifulSoup
from fpdf import FPDF
from markdownify import markdownify
from playwright.async_api import async_playwright

OUTPUT_DIR = Path("site_manual")
SCREENSHOT_DIR = OUTPUT_DIR / "screenshots"
MARKDOWN_DIR = OUTPUT_DIR / "markdown"
FONT_PATH = Path("DejaVuSans.ttf")  # Path to downloaded font


def slugify(text: str) -> str:
    """Sanitize text for cross-platform filenames.

    Replaces sequences of non-alphanumeric characters with underscores so
    generated paths work consistently on Windows and POSIX systems.
    """
    return re.sub(r"[^0-9A-Za-z]+", "_", text).strip("_")


def is_same_domain(url: str, base_url: str) -> bool:
    return urlparse(url).netloc == urlparse(base_url).netloc


async def crawl_site(base_url: str):
    """Breadth-first crawl of pages within base_url."""
    visited, pages = set(), []
    queue = deque([base_url])

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        while queue:
            url = queue.popleft()
            if url in visited:
                continue
            print(f"Crawling {url}")
            visited.add(url)

            page = await browser.new_page()
            await page.goto(url, wait_until="networkidle")
            html = await page.content()
            await page.close()
            soup = BeautifulSoup(html, "html.parser")
            pages.append((url, soup))

            for link in soup.select("a[href]"):
                full = urljoin(url, link["href"])
                if full.startswith("http") and is_same_domain(full, base_url):
                    queue.append(full)
        await browser.close()

    return pages


async def take_snapshots(pages):
    """Headless browser to capture full-page screenshots."""
    SCREENSHOT_DIR.mkdir(parents=True, exist_ok=True)
    results = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        for idx, (url, soup) in enumerate(pages, 1):
            page = await browser.new_page()
            await page.goto(url)
            screenshot = SCREENSHOT_DIR / f"{idx:03}.png"
            await page.screenshot(path=str(screenshot), full_page=True)
            await page.close()
            results.append((idx, url, soup, screenshot))
        await browser.close()

    return results


def build_docs(entries):
    """Create PDF and Markdown files describing each page."""
    MARKDOWN_DIR.mkdir(parents=True, exist_ok=True)
    pdf = FPDF()
    if not FONT_PATH.exists():
        raise FileNotFoundError(f"Font file not found: {FONT_PATH}")
    pdf.add_font("DejaVu", "", str(FONT_PATH), uni=True)

    for idx, url, soup, img_path in entries:
        title = soup.title.string.strip() if soup.title else url
        text = markdownify(str(soup.body)) if soup.body else ""
        md_content = f"# {title}\n\nURL: {url}\n\n{text}"

        # Markdown manual
        md_file = MARKDOWN_DIR / f"{idx:03}_{slugify(title)}.md"
        md_file.write_text(md_content, encoding="utf-8")

        # PDF section
        pdf.add_page()
        pdf.set_font("DejaVu", size=14)
        pdf.multi_cell(0, 8, md_content)
        pdf.ln(5)
        try:
            pdf.image(str(img_path), w=180)
        except OSError as exc:
            logging.warning(
                "Skipping image %s because Pillow is not installed or failed to read it: %s",
                img_path,
                exc,
            )

    pdf.output(str(OUTPUT_DIR / "manual.pdf"))


def main(url: str):
    pages = asyncio.run(crawl_site(base_url=url))
    entries = asyncio.run(take_snapshots(pages))
    build_docs(entries)


if __name__ == "__main__":
    base_url = "http://localhost:5173/"  # Change to your server

    main(url=base_url)
